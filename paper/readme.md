# 论文分析/笔记

## 1_SanaLabs
### 特征工程
除去已经给出的特征，又额外构建了下面特征
- 单词已经练习的次数
- 上一次这个单词出现到现在的时间
- 这个单词在答案句中的位置
- 这个单词在答案句中的次数
- 这次练习在session中的位置
- 前一个单词
- 答案句的唯一标识



### 模型
动机:GBDT在多维度数据预测上一直有最佳结果，RNN能有效对序列建模

集合模型: GBDT+RNN(LSTM)

final = a*GBDT_result + (1-a) * RNN_result

权重a和准确率是先增加后先减少，并在a为0.7附近，获得最大值

### 实验结果

RNN 和 GBDT 在每个单词上的表现不一样，也就说有的单词上RNN表现好，有的单词上DBDT好。
总体效果，是排第一的，并且单个模型效果都不会是第一。 

最有可能的是，RNN从时间动力角度补充了GBDT

特征前三: token,user_id,format

预测错误情况的分析：

1. 模型没有学习到每个用户最近时间的错误信息，训练集上的数据时间比较久

2. 生僻单词

### 疑问
特征工程的意义？为什么不利用神经网络自动学习特征？

## 2_singsound

### 特征工程
将特征分成4类：上下文特征，语言学特征，用户特征和场景特征

### 模型

#### Context Encoder

分成单词级和字符级两个网络，核心都是双向的LSTM网络

动机: 单词级能够捕获语义学知识，字符级可以用来学习新词

编码器最终输出是，两个网络的输出的加和 

#### Linguistic Feature Encode

一个LSTM网络

#### User Encoder / Format Encoder

一层全连接的网络，激活函数:tanh

#### Decoder

三层连接的网络，激活函数:sigmoid

优化算法 Adam

### 实验结果
- 上下文编码器 最重要
- 语言学编码器 最不重要

### 疑问

Context Encoder中的输出结果，尝试带权重的相加？


## 3_NYU
### 特征工程
动机: 利用心理学上的理论来构造特征
#### 单词特征
句子根单词，单词， 词性，形态，依存关系，单词长度，其他外部数据源得到的单词特征

#### 用户特征
user_id,学习欲望,勤勉程度

学习欲望:在一段时间内(1小时)是不是完成了更多的练习

勤勉程度:是不是在每天相同的时间段进行练习

#### 位置特征
前一个单词，后一个单词，在依存树中的根节点单词，上下文单词的词性

#### 时间特征
用户第几次遇到这个单词，单词错误情况

### 模型
GBDT

动机:GBDT在各种数据集中表现良好，可以提取特征之间复杂的关系，并且相对于深度学习速度更快，更方便的集成。

### 实验结果
- user_id 最重要
- 除去user_id其他的user信息不重要

模型之所以成功不是因为手工构造了这么多别的特征，而是依赖于GBDT的强大。即将原始的特征输入到GDBT也能有非常好的效果

所以在数据集较大的情况下，通过心理学知识手工构建特征并不是好的方案

原因:

1. 计算心理模型通常基于小型实验室数据集来设计，具备可解释性。所以其本质不是预测而是解释！！所以就是有限的特征和它们之间有限的交互。

    而GDBT和深度学习的模型能够自动学习上百的特征之间的高级交互，并进行预测，虽然模型是不透明的。
    
2. 大规模的数据下，心理学得到的结论可能并不适用

3. 在本任务中，可以基于用户ID，来对每个用户进行训练，也就是个人定制，但是对于新的用户，这个模型就会失效，但是对于心理学上的特征，就会依然有效。 冷启动问题？

开源了代码: https://github.com/NYUCCL/duolingoSLAM

### 疑问
冷启动问题？训练数据中，该用户的数据不足，可以结合心理学知识？


## 4_TMU
### 特征工程
构造了两个额外的特征:
1. 学习历史，History LSTM的最后一个隐藏层
2. 语言标识

### 模型
Bi-LSTM + LSTM

### 实验结果
- 学习历史这个额外信息非常重要，加入/不加入 差距有20%

### 疑问
History LSTM 是怎么作用于 预测的 ？


## 5_CECL
### 特征工程
- 前2个，后两个和单词本身 5个
- 数据集提供的元数据 7个
- 前两者的两两结合 35个
一共47个特征

### 模型
L1-LR / L2-LR

### 实验结果
- 结合的特征对模型提升有很大的作用
- 单词特征和元特征结合的新特征对模型提升很大
- session相关的特征对模型影响不大

特点是 没有使用序列信息

### 疑问
前面的GBDT是否也可以认为是没有使用序列信息？？

## 6_Cambridge
### 特征工程
- CEFR 语言熟练度
- CLC 错误率


### 模型
sequence labelling : 双向LSTM + 一层全神经网络 + softmax

sequence-to-sequence modelling: 预测结果通过 输入x 和前面的预测结果y_t-1确定


### 实验结果
CEFR 和 CLC 并没有提供有用的信息，加入以后，反而降低了正确率

把两个模型有权重的相加，实验效果最好

### 疑问
特征工程的意义是......


## 7_UCSD
### 特征工程
- 语言学特征:单词长度/语义相近的单词(外部数据)/单词频率/Levenshtein距离/是否是同型词/形态复杂性/具体性
- 记忆特征: 单词遇见次数/所有遇到过的单词数/上一次遇见到现在的时间
- 对用户和单词进一步构造多维特征


### 模型
随机森林

### 实验结果
- 用户特征 和 单词 特征有用

### 疑问


### 8_LambdaLab
### 特征工程
自己构建了16个特征


### 模型
GDBT

### 实验结果
- 重复次数特征有一定的作用

## 9_Grotoco
### 特征工程
自己构建了额外的特征，用户遇见这个单词的次数


### 模型
L2-LR

### 实验结果
- 移除第一天的数据，效果会更好！！第一天出错和软件使用本身有关和语言本身无关
- 用户特征比较重要

## 10_nihalnayak
### 特征工程
User + Format

语言学的特征


### 模型
逻辑回归

### 模型
文本简化技术是否可以用来简化句子，帮助用户学习语言

开源了代码: https://github.com/iampuntre/slam18


## 11_jilljenn

### 特征工程
选取了一部分特征作为基础特征，有确定的值，用户资料相关

选取了一部分特征作为不确定特征 语义学相关

选取了一部分特征作为连续特征 time 

### 模型

DeepFM : FM + Deep component(L层神经网络 )


### 实验结果
仅仅采用基础特征，效果最好

开源了代码: https://github.com/jilljenn/ktm

